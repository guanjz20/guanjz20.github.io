<!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"> -->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="keywords" content="StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator; StyleSync; Lip Sync; Talking Face Generation; Deep learning;">
  <meta name="description" content="
  Despite recent advances in syncing lip movements with any audio waves, current methods still struggle to balance generation quality and the model's generalization ability.
Previous studies either require long-term data for training or produce a similar movement pattern on all subjects with low quality.
In this paper, we propose StyleSync, an effective framework that enables high-fidelity lip synchronization. We identify that a style-based generator would sufficiently enable such a charming property on both one-shot and few-shot scenarios.
Specifically, we design a mask-guided spatial information encoding module that preserves the details of the given face. The mouth shapes are accurately modified by audio through modulated convolutions.
Moreover, our design also enables personalized lip-sync by introducing style space and generator refinement on only limited frames. Thus the identity and talking style of a target person could be accurately preserved.
Extensive experiments demonstrate the effectiveness of our method in producing high-fidelity results on a variety of scenes.
  ">
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <link rel="stylesheet" href="css.css" type="text/css">
  <link rel="stylesheet" href="project.css" type="text/css" media="screen">
  <script async="" src="prettify.js"></script>

  <style>
    body {
      background-color: #f8f9fa;
      font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
      color: #2c3e50;
      margin: 0;
      padding: 20px;
    }

    #content {
      max-width: 1200px;
      margin: 0 auto;
      background: #ffffff;
      border: 1px solid #e1e8ed;
      border-radius: 16px;
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.08), 0 2px 12px rgba(0, 0, 0, 0.04);
      overflow: hidden;
      position: relative;
    }

    #content::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 3px;
      background: linear-gradient(90deg, #3498db 0%, #5dade2 50%, #3498db 100%);
      border-radius: 16px 16px 0 0;
    }

    #content-inner {
      padding: 30px;
    }

    .head {
      text-align: center;
      padding: 30px 0;
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      margin: -30px -30px 30px -30px;
      border-radius: 16px 16px 0 0;
    }

    .head h1 {
      font-size: 2.2rem;
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 20px;
      line-height: 1.3;
      padding: 0 40px;
    }

    .authors {
      font-size: 1.1rem;
      color: #5d6d7e;
      margin-bottom: 15px;
      line-height: 1.6;
    }

    .authors a {
      color: #3498db;
      text-decoration: none;
      border-bottom: 1px dotted transparent;
      transition: border-color 0.2s ease;
    }

    .authors a:hover {
      border-bottom: 1px dotted #3498db;
    }

    .affiliations {
      font-size: 0.95rem;
      color: #7f8c8d;
      margin-bottom: 15px;
      line-height: 1.5;
    }

    .venue {
      font-size: 1.1rem;
      font-weight: 500;
      color: #3498db;
      margin-top: 15px;
    }

    .section {
      margin: 40px 0;
    }

    .section h1 {
      font-size: 1.8rem;
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 20px;
      padding-bottom: 10px;
      border-bottom: 2px solid #3498db;
    }

    .section h3 {
      font-size: 1.3rem;
      font-weight: 500;
      color: #34495e;
      margin: 30px 0 15px 0;
    }

    .abstract {
      font-size: 1rem;
      line-height: 1.7;
      color: #5d6d7e;
      background: #f8f9fa;
      padding: 25px;
      border-radius: 12px;
      border: 1px solid #e1e8ed;
      margin: 20px 0;
    }

    .bibtex pre {
      background: #f8f9fa;
      border: 1px solid #e1e8ed;
      border-radius: 8px;
      padding: 20px;
      font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
      font-size: 0.9rem;
      color: #5d6d7e;
      overflow-x: auto;
      margin: 20px 0;
    }

    .griditem {
      background: #ffffff;
      border: 1px solid #e1e8ed;
      border-radius: 12px;
      padding: 20px;
      text-align: center;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
    }

    .griditem:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
    }

    .griditem img {
      border-radius: 8px;
      margin-bottom: 10px;
      border: 1px solid #e1e8ed;
    }

    .griditem a {
      color: #3498db;
      text-decoration: none;
      font-weight: 500;
      border-bottom: 1px dotted transparent;
      transition: border-color 0.2s ease;
    }

    .griditem a:hover {
      border-bottom: 1px dotted #3498db;
    }

    .demo {
      text-align: center;
      margin: 40px 0;
    }

    .demo iframe {
      border-radius: 12px;
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.12);
      border: 1px solid #e1e8ed;
    }

    @media (max-width: 768px) {
      body {
        padding: 10px;
      }
      
      #content-inner {
        padding: 20px;
      }
      
      .head h1 {
        font-size: 1.8rem;
        padding: 0 20px;
      }
      
      .authors {
        font-size: 1rem;
      }
      
      .section h1 {
        font-size: 1.5rem;
      }
      
      .abstract {
        padding: 20px;
      }
    }
  </style>

  <title>StyleSync</title>
  <!-- <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39824124-1']);
    _gaq.push(['_trackPageview']);

    (function () {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script> -->

</head>


<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>StyleSync: High-Fidelity Generalized and Personalized <br>
          Lip Sync in Style-based Generator</h1>

        <div class="authors">
          <a href="https://guanjz20.github.io/">Jiazhi Guan</a><sup>1,2*</sup>&nbsp;&nbsp;
          Zhanwang Zhang<sup>1*</sup>&nbsp;&nbsp;
          <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a><sup>1†</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=BIixVT0AAAAJ">Tianshu Hu</a><sup>1†</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ">Kaisiyuan Wang</a><sup>3</sup>&nbsp;&nbsp;

          <br>
          <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ">Dongliang He</a><sup>2</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=pnuQ5UsAAAAJ&view_op=list_works&sortby=pubdate">Haocheng Feng</a><sup>1</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=tVV3jmcAAAAJ">Jingtuo Liu<sup>1</sup>&nbsp;&nbsp;
            <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ">Errui Ding</a><sup>1</sup>&nbsp;&nbsp;
            <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>4</sup>&nbsp;&nbsp;
            <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>1</sup>
        </div>

        <div class="affiliations">

          1. Department of Computer Vision Technology (VIS), Baidu Inc.,&nbsp;&nbsp;
          2. Tsinghua University,<br>
          3. The University of Sydney,&nbsp;&nbsp;
          4. S-Lab, Nanyang Technological University.
        </div>

        <div class="venue">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a
            href="https://cvpr.thecvf.com/Conferences/2023">CVPR</a>) 2023 </div>
      </div>

      <center><img src="pipeline.png" border="0" width="90%"></center>

      <div class="section abstract">
        <h1>Abstract</h1>
        <br>
        <p>
          Despite recent advances in syncing lip movements with any audio waves, current methods still struggle to
          balance generation quality and the model's generalization ability.
          Previous studies either require long-term data for training or produce a similar movement pattern on all
          subjects with low quality.
          In this paper, we propose StyleSync, an effective framework that enables high-fidelity lip synchronization. We
          identify that a style-based generator would sufficiently enable such a charming property on both one-shot and
          few-shot scenarios.
          Specifically, we design a mask-guided spatial information encoding module that preserves the details of the
          given face. The mouth shapes are accurately modified by audio through modulated convolutions.
          Moreover, our design also enables personalized lip-sync by introducing style space and generator refinement on
          only limited frames. Thus the identity and talking style of a target person could be accurately preserved.
          Extensive experiments demonstrate the effectiveness of our method in producing high-fidelity results on a
          variety of scenes.
        </p>
      </div>

      <div class="section demo">
        <h1>Demo Video</h1>
        <br>
        <center>
          <iframe width="640" height="360" src="https://www.youtube.com/embed/uuBglL2KGFc" frameborder="0"
            allowfullscreen></iframe>
          </video>
        </center>
      </div>

      <div class="section materials">
        <h1>Materials</h1>
        <center>
          <ul>
            <li class="grid">
              <div class="griditem">
                <a href="https://arxiv.org/pdf/2305.05445.pdf" class="imageLink"><img
                    src="paper.png"></a>
                <br>
                <a href="https://arxiv.org/pdf/2305.05445.pdf">Paper</a>
              </div>
            </li>

            <!--		  <li class="grid">-->
            <!--	      <div class="griditem">-->
            <!--		<a href="https://drive.google.com/file/d/182roBLw9NChEugMHuLLkmuElifFYhQaq/view" class="imageLink"><img src="poster.png"></a><br>-->
            <!--		  <a href="https://drive.google.com/file/d/182roBLw9NChEugMHuLLkmuElifFYhQaq/view">Poster</a>-->
            <!--		</div>-->
            <!--	      </li>-->

            <li class="grid">
              <div class="griditem">
                <a href="https://github.com/guanjz20/StyleSync" class="imageLink"><img
                    src="code.png"></a>
                <br>
                <a href="https://github.com/guanjz20/StyleSync">Code</a>
              </div>
            </li>

          </ul>
        </center>
      </div>

      <div class="section citation">
        <h1>Citation</h1>
        <div class="section bibtex">
          <pre>
@inproceedings{guan2023stylesync,
  title = {StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator},
  author = {Guan, Jiazhi and Zhang, Zhanwang and Zhou, Hang and HU, Tianshu and Wang, Kaisiyuan and He, Dongliang and Feng, Haocheng and Liu, Jingtuo and Ding, Errui and Liu, Ziwei and Wang, Jingdong},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2023}
}</pre>
          <br>
        </div>
      </div>

      
</body>


</html>