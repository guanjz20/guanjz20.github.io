<!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"> -->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="keywords"
    content="AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers; AudCast; Human Video Generation; Deep learning;">
  <meta name="description" content="
  Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results.  Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details. 
  ">
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <link rel="stylesheet" href="css.css" type="text/css">
  <link rel="stylesheet" href="project.css" type="text/css" media="screen">
  <script async="" src="prettify.js"></script>

  <style>
    body {
      background-color: #f8f9fa;
      font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
      color: #2c3e50;
      margin: 0;
      padding: 20px;
    }

    #content {
      max-width: 1200px;
      margin: 0 auto;
      background: #ffffff;
      border: 1px solid #e1e8ed;
      border-radius: 16px;
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.08), 0 2px 12px rgba(0, 0, 0, 0.04);
      overflow: hidden;
      position: relative;
    }

    #content::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 3px;
      background: linear-gradient(90deg, #3498db 0%, #5dade2 50%, #3498db 100%);
      border-radius: 16px 16px 0 0;
    }

    #content-inner {
      padding: 30px;
    }

    .head {
      text-align: center;
      padding: 30px 0;
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      margin: -30px -30px 30px -30px;
      border-radius: 16px 16px 0 0;
    }

    .head h1 {
      font-size: 2.2rem;
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 20px;
      line-height: 1.3;
      padding: 0 40px;
    }

    .authors {
      font-size: 1.1rem;
      color: #5d6d7e;
      margin-bottom: 15px;
      line-height: 1.6;
    }

    .authors a {
      color: #3498db;
      text-decoration: none;
      border-bottom: 1px dotted transparent;
      transition: border-color 0.2s ease;
    }

    .authors a:hover {
      border-bottom: 1px dotted #3498db;
    }

    .affiliations {
      font-size: 0.95rem;
      color: #7f8c8d;
      margin-bottom: 15px;
      line-height: 1.5;
    }

    .venue {
      font-size: 1.1rem;
      font-weight: 500;
      color: #3498db;
      margin-top: 15px;
    }

    .section {
      margin: 40px 0;
    }

    .section h1 {
      font-size: 1.8rem;
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 20px;
      padding-bottom: 10px;
      border-bottom: 2px solid #3498db;
    }

    .section h3 {
      font-size: 1.3rem;
      font-weight: 500;
      color: #34495e;
      margin: 30px 0 15px 0;
    }

    .abstract {
      font-size: 1rem;
      line-height: 1.7;
      color: #5d6d7e;
      background: #f8f9fa;
      padding: 25px;
      border-radius: 12px;
      border: 1px solid #e1e8ed;
      margin: 20px 0;
    }

    .bibtex pre {
      background: #f8f9fa;
      border: 1px solid #e1e8ed;
      border-radius: 8px;
      padding: 20px;
      font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
      font-size: 0.9rem;
      color: #5d6d7e;
      overflow-x: auto;
      margin: 20px 0;
    }

    .griditem {
      background: #ffffff;
      border: 1px solid #e1e8ed;
      border-radius: 12px;
      padding: 20px;
      text-align: center;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
    }

    .griditem:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
    }

    .griditem img {
      border-radius: 8px;
      margin-bottom: 10px;
      border: 1px solid #e1e8ed;
    }

    .griditem a {
      color: #3498db;
      text-decoration: none;
      font-weight: 500;
      border-bottom: 1px dotted transparent;
      transition: border-color 0.2s ease;
    }

    .griditem a:hover {
      border-bottom: 1px dotted #3498db;
    }

    video, img {
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
      border: 1px solid #e1e8ed;
    }

    .demo {
      text-align: center;
      margin: 40px 0;
    }

    .demo iframe {
      border-radius: 12px;
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.12);
      border: 1px solid #e1e8ed;
    }

    .results {
      margin: 40px 0;
    }

    .results b {
      color: #2c3e50;
      font-weight: 600;
    }

    @media (max-width: 768px) {
      body {
        padding: 10px;
      }
      
      #content-inner {
        padding: 20px;
      }
      
      .head h1 {
        font-size: 1.8rem;
        padding: 0 20px;
      }
      
      .authors {
        font-size: 1rem;
      }
      
      .section h1 {
        font-size: 1.5rem;
      }
      
      .abstract {
        padding: 20px;
      }
    }
  </style>

  <title>AudCast</title>
  <!-- <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39824124-1']);
    _gaq.push(['_trackPageview']);

    (function () {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script> -->

</head>



<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers
        </h1>

        <div class="authors">
          <a href="https://guanjz20.github.io/">Jiazhi Guan</a><sup>1,2</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ">Kaisiyuan Wang</a><sup>2</sup>&nbsp;&nbsp;
          <a href="https://seanseattle.github.io/">Zhiliang Xu</a><sup>2</sup>&nbsp;&nbsp;
          Quanwei Yang<sup>5</sup>&nbsp;&nbsp;
          <a href="https://sunyasheng.github.io/">Yasheng Sun</a><sup>6</sup>&nbsp;&nbsp;
          Shengyi He<sup>2</sup>&nbsp;&nbsp;
          <br>
          <a href="https://scholar.google.com/citations?user=miLIl2gAAAAJ&hl=en">Borong
            Liang</a><sup>2</sup>&nbsp;&nbsp;
          <a href="https://yukangcao.github.io/">Yukang Cao</a><sup>3</sup>&nbsp;&nbsp;
          Yingying Li<sup>2</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=pnuQ5UsAAAAJ&view_op=list_works&sortby=pubdate">Haocheng
            Feng</a><sup>2</sup>&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ">Errui Ding</a><sup>2</sup>&nbsp;&nbsp;
          <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>2</sup>&nbsp;&nbsp;
          <br>
          <a href="https://www.cs.tsinghua.edu.cn/csen/info/1309/4355.htm">Youjian Zhao</a><sup>1,4†</sup>&nbsp;&nbsp;
          <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a><sup>2†</sup>&nbsp;&nbsp;
          <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>3†</sup>&nbsp;&nbsp;
        </div>

        <div class="affiliations">
          1. DCST, Tsinghua University,&nbsp;&nbsp;
          2. Baidu Inc.,&nbsp;&nbsp;
          3. S-Lab, Nanyang Technological University,
          <br>
          4. Zhongguancun Laboratory,&nbsp;&nbsp;
          5. University of Science and Technology of China,&nbsp;&nbsp;
          6. KAUST.
        </div>

        <div class="venue">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a
          href="https://cvpr.thecvf.com/Conferences/2025">CVPR</a>) 2025 </div>

      </div>

      <center><img src="pipeline.png" border="0" width="90%"></center>

      <div class="section abstract">
        <h1>Abstract</h1>
        <br>
        <p>
          Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial
          movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to
          generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio.
          In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a
          cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference
          image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to
          directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face
          details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as
          the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our
          framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial
          and hand details.
        </p>
      </div>


      <!-- <div class="section demo">
        <h1>Demo Video</h1>
        <br>
        <center>
          <iframe width="640" height="360" src="https://www.youtube.com/embed/" frameborder="0"
            allowfullscreen></iframe>
          </video>
        </center>
      </div> -->


      <div class="section results">
        <h1>Results</h1>

        <h3>Comparions with S2G</h3>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 20px;">
          <!-- set 1             -->
          <div style="display: flex; flex-direction: row; align-items: center;">
            <div style="display: flex; flex-direction: column; align-items: center">
              <video controls style="width: 896px; height: 500px; display: block; object-fit: contain;">
                <source src="video/cmp_s2g.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
        <br>

        <h3>Comparions with Audio2Gesture2Video Baselines</h3>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 20px;">
          <!-- set 1             -->
          <div style="display: flex; flex-direction: row; align-items: center;">
            <div style="display: flex; flex-direction: column; align-items: center">
              <video controls style="width: 896px; height: 500px; display: block; object-fit: contain;">
                <source src="video/cmp_2stage_baseline.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
        <br>

        <h3>Comparions with Vlogger's Demo Videos</h3>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 20px;">
          <!-- set 1             -->
          <div style="display: flex; flex-direction: row; align-items: center;">
            <div style="display: flex; flex-direction: column; align-items: center">
              <video controls style="width: 896px; height: 500px; display: block; object-fit: contain;">
                <source src="video/cmp_vlogger.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
        <br>

        <h3>Comparions with Cyberhost's Demo Videos</h3>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 20px;">
          <!-- set 1             -->
          <div style="display: flex; flex-direction: row; align-items: center;">
            <div style="display: flex; flex-direction: column; align-items: center">
              <video controls style="width: 896px; height: 500px; display: block; object-fit: contain;">
                <source src="video/cmp_cyberhost.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      
      </div>


      <div class="section materials">
        <h1>Materials</h1>
        <center>
          <ul>
            <li class="grid">
              <div class="griditem">
                <a href="https://arxiv.org/abs/2503.19824" class="imageLink"><img src="paper.png"></a>
                <br>
                <a href="https://arxiv.org/abs/2503.19824">Paper</a>
              </div>
            </li>

          </ul>
        </center>
      </div>

      <div class="section citation">
        <h1>Citation</h1>
        <div class="section bibtex">
          <pre>
@inproceedings{guan2024resyncer,
  title = {AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers},
  author = {Guan, Jiazhi and  Wang, Kaisiyuan and Xu, Zhiliang and Yang, Quanwei and Sun, Yasheng and He, Shengyi and Liang, Borong and Cao, Yukang and Li, Yingying and Feng, Haocheng and Ding, Errui and Wang, Jingdong and Zhao, Youjian and Zhou, Hang and Liu, Ziwei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025}
}</pre>
          <br>
        </div>
      </div>


</body>


</html>